=================================================================================
FULL-FEATURED RESEARCH PIPELINE GUIDE
PKM GraphRAG System - Step-by-Step Research Workflow
=================================================================================

Date: December 9, 2025
System Version: v2.1 with Graph-Guided Retrieval (Phase 1 Complete)
Compliance: 75/100

This guide walks you through a complete research workflow using all available
tools in the codebase, from document collection to knowledge graph analysis.

=================================================================================
OVERVIEW: Available Tools
=================================================================================

1. Document Processing
   - PDF extraction (via pypdf)
   - HTML/web article extraction (via BeautifulSoup + html2text)
   - Markdown and plain text files
   - YouTube transcript extraction
   - âœ… DOCX support (via python-docx) - NEWLY ADDED!

2. Knowledge Graph Building
   - Automatic chunking (paragraph-based, ~500 tokens)
   - Concept extraction (headings + NER-like patterns)
   - Topic generation (auto-clustering)
   - RDF/Turtle export with full ontology

3. Retrieval & Chat
   - Graph-guided retrieval (topic â†’ concept â†’ chunk)
   - Hybrid search (graph traversal + vector similarity)
   - Full retrieval path transparency
   - Zero-hallucination responses with citations

4. Research Notebooks
   - Web discovery workflow (source_discovery.ipynb)
   - Research workflow (research_workflow.ipynb)

5. Article Generation
   - Generate synthesis articles from knowledge graph
   - AI-powered narrative creation from RDF triples

6. Flask Web UI
   - Simple chat interface (localhost:5000)
   - Real-time Q&A with your knowledge base

=================================================================================
DOCX FILES NOW SUPPORTED! âœ…
=================================================================================

ðŸŽ‰ GOOD NEWS: The system NOW supports .docx files via auto-conversion to TXT!

Your file: LinkedIn+Blog_Posts_Set_1_of_3_EU_Data_Act_Linked_Data.docx
Status: âœ… WILL BE AUTO-CONVERTED TO TXT

HOW IT WORKS:

1. When you place a .docx file in data/sources/
2. The system automatically converts it to .txt format (same directory)
3. The .txt file is used for all processing (RAG, graph building, chat)
4. Conversion happens once (cached until .docx is modified)

SETUP (One-Time):

Step 1: Install python-docx library
  pip install python-docx

Step 2: Verify installation
  python -c "from docx import Document; print('âœ“ python-docx installed')"

USAGE:

Simply place .docx files in data/sources/ - they will auto-convert to .txt!

Example:
  data/sources/
    â”œâ”€â”€ my_document.docx          â† Your original file
    â”œâ”€â”€ my_document.txt           â† Auto-generated (used by system)
    â”œâ”€â”€ other_file.pdf
    â””â”€â”€ notes.md

The system will:
  âœ“ Extract paragraphs from DOCX
  âœ“ Extract tables (formatted with pipes: cell1 | cell2 | cell3)
  âœ“ Save as .txt file
  âœ“ Use .txt for all processing (faster, more reliable)
  âœ“ Re-convert only if .docx is modified

BENEFITS:

1. Reliable parsing (TXT is simpler than DOCX)
2. Faster processing (no repeated DOCX parsing)
3. Human-readable output (you can check the .txt file)
4. Version tracking (TXT files can be git-committed)

NOTE: The .txt files are auto-generated. You can safely delete them - they'll
      be recreated from the .docx on next run.

=================================================================================
STEP-BY-STEP RESEARCH PIPELINE
=================================================================================

PHASE 1: DOCUMENT COLLECTION & PREPARATION
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Step 1.1: Install python-docx (One-Time Setup)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Tool: pip
Command:
  pip install python-docx

Verification:
  python -c "from docx import Document; print('âœ“ DOCX support ready')"

Expected Time: 30 seconds

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Step 1.2: Verify Your DOCX File is Ready
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Tool: PowerShell
File: data/sources/LinkedIn+Blog_Posts_Set_1_of_3_EU_Data_Act_Linked_Data.docx

Command:
  Get-Item "data/sources/LinkedIn+Blog_Posts_Set_1_of_3_EU_Data_Act_Linked_Data.docx"

Expected Output:
  Shows file exists with size and modification date

Action: âœ… No manual conversion needed! File will auto-convert to TXT.

Expected Time: 10 seconds

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Step 1.3: Review All Existing Sources
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Tool: PowerShell
Command:
  Get-ChildItem data/sources | Select-Object Name, Length, LastWriteTime | Format-Table -AutoSize

Current Sources (as of test):
  - Cloud and Edge Computing.pdf
  - Cloud switching under the EU Data Act.pdf
  - Knowledge Graphs.pdf
  - node_12633_printable_pdf.pdf
  - The EU Data Act explained.pdf
  - tr_104410v010101p.pdf
  - YouTube-8nbb6CwpPMA.md
  - RDF Levels the Advantages.html (+ support files)
  - knowledge_graph_article.md (previously generated)

Total: ~18 files

Expected Time: 1 minute

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

PHASE 2: WEB RESEARCH & SOURCE DISCOVERY (OPTIONAL)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Step 2.1: Open Source Discovery Notebook
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Tool: Jupyter Notebook
File: notebooks/source_discovery.ipynb

Command:
  jupyter notebook notebooks/source_discovery.ipynb

What it does:
  - Helps generate search queries for your research topic
  - Allows you to paste URLs of relevant articles
  - Automatically extracts content from web pages
  - Saves articles as Markdown files in data/sources/

Workflow:
  1. Define research topic (e.g., "EU Data Act implementation")
  2. Generate search queries (AI-assisted)
  3. Paste URLs of articles you found
  4. Notebook extracts content and saves to data/sources/
  5. Articles auto-tagged with frontmatter

Example:
  research_topic = "EU Data Act compliance for IoT manufacturers"
  # â†’ Generates 10 search queries
  # â†’ Paste URLs: https://example.com/article1, https://...
  # â†’ Extracts and saves to data/sources/article_extracted_20251209.md

Expected Time: 15-30 minutes (depending on number of sources)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Step 2.2: YouTube Transcript Processing (If Applicable)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Tool: process_youtube.py
File: data/sources/youtube_links.txt

Setup:
  1. Create/edit: data/sources/youtube_links.txt
  2. Add YouTube URLs (one per line):
     https://www.youtube.com/watch?v=VIDEO_ID_1
     https://www.youtube.com/watch?v=VIDEO_ID_2

Command Option A (Preserve Timestamps):
  python process_youtube.py

  Output: YouTube-VIDEO_ID.md with timestamped transcript

Command Option B (AI Article Conversion):
  python process_youtube.py --article

  Output: YouTube-VIDEO_ID.md as narrative article

Verification:
  Get-ChildItem data/sources/YouTube-*.md

Expected Time: 5-10 minutes + processing time

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

PHASE 3: KNOWLEDGE GRAPH CONSTRUCTION
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Step 3.1: Build Knowledge Graph with All Features
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Tool: build_graph.py
Command:
  python build_graph.py

What happens:
  1. Loads all documents from data/sources/
     - PDF, HTML, Markdown, TXT files
     - Frontmatter parsing
     - MIME type detection

  2. Splits into semantic chunks
     - Paragraph-based splitting
     - Target: ~500 tokens per chunk
     - Position tracking (chunkIndex)

  3. Extracts domain concepts
     - From headings (H1-H6)
     - NER-like capitalized phrases
     - Quality filtering (2-5 word phrases)

  4. Generates topic nodes
     - Auto-clustering (10 concepts per topic)
     - Creates navigation layer
     - 100% concept coverage guaranteed

  5. Builds RDF graph
     - 3 layers: Chunk, DomainConcept, TopicNode
     - SKOS/DCTERMS/RDFS ontology
     - Bidirectional relationships

  6. Exports to Turtle (TTL)
     - Human-readable format
     - Comprehensive 27-line header
     - Clean labels, descriptive comments

Output:
  data/graphs/knowledge_graph.ttl

Expected Stats (with your sources):
  - Documents: ~18-19
  - Chunks: ~25-30
  - Concepts: ~120-150
  - Topics: ~12-15
  - Total Triples: ~800-1000

Expected Time: 5-10 seconds

Verification:
  Get-Content data/graphs/knowledge_graph.ttl -Head 50
  # Should see header with statistics

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Step 3.2: Review Graph Statistics
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Tool: build_graph.py output

Look for:
  ðŸ“Š Graph Statistics:
    â€¢ total_triples: 800+
    â€¢ documents: 18-19
    â€¢ chunks: 25-30
    â€¢ domain_concepts: 120-150
    â€¢ topic_nodes: 12-15
    â€¢ chunk_mentions: (concepts linked to chunks)
    â€¢ topic_covers_concepts: (concepts organized into topics)

Quality Checks:
  âœ“ All documents loaded? (Check "Warning" messages for failures)
  âœ“ Reasonable number of chunks? (~1-2 per document)
  âœ“ Concepts extracted? (Should be 5-10x number of documents)
  âœ“ Topics generated? (Should be ~10-15% of concepts)

Expected Time: 2 minutes

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Step 3.3: Inspect TTL File (Optional)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Tool: Text editor or ProtÃ©gÃ© (RDF editor)

Commands:
  # View in text editor
  code data/graphs/knowledge_graph.ttl

  # Or use grep to find specific concepts
  Select-String -Path data/graphs/knowledge_graph.ttl -Pattern "EU Data Act"

Look for:
  - TopicNode instances with skos:prefLabel
  - DomainConcept instances with rdfs:comment
  - Chunk instances with chunkText
  - Relationships: coversConcept, mentionsConcept, hasChunk

Expected Time: 5-10 minutes (if inspecting)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

PHASE 4: INTERACTIVE RESEARCH & QUERYING
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Step 4.1: Test Graph-Guided Retrieval
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Tool: test_chat.py
Command:
  python test_chat.py

What happens:
  1. Loads all documents
  2. Builds knowledge graph in memory
  3. Asks test question: "what is the project about?"
  4. Uses graph-guided retrieval:
     - Maps query to topics (via embeddings)
     - Traverses to concepts under topics
     - Finds chunks mentioning concepts
     - Ranks by hybrid score (topic + vector)
  5. Generates answer with full citations
  6. Displays retrieval path transparency

Output Example:
  ðŸ” Graph Retrieval Path:

    ðŸ“Š Topics Activated:
      â€¢ Topic: EU Data Act Implementation (relevance: 0.342)
      â€¢ Topic: Linked Data Standards (relevance: 0.298)

    ðŸ§  Concepts Matched:
      â€¢ EU Data Act
      â€¢ Linked Data
      â€¢ Knowledge Graphs
      â€¢ Interoperability

    ðŸ“„ Retrieved 5 text chunks from knowledge graph

  ðŸ“š Sources:
    [1] LinkedIn Blog Posts - EU Data Act...
        ðŸ“„ LinkedIn_Blog_Posts_EU_Data_Act.md
        ðŸ”¢ Chunk: 2
        ðŸ“Š Relevance: 0.456

Expected Time: 10-15 seconds

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Step 4.2: Interactive Chat Session
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Tool: features/chat.py (VaultChat)
Command:
  python -c "from features.chat import VaultChat; chat = VaultChat(verbose=True); chat.rag.build_knowledge_graph(enable_chunking=True, enable_topics=True); chat.interactive_session()"

Or create a simple script: interactive_chat.py:
  from features.chat import VaultChat

  chat = VaultChat(verbose=True)
  chat.rag.build_knowledge_graph(enable_chunking=True, enable_topics=True)
  chat.interactive_session()

Then run:
  python interactive_chat.py

Features:
  - Ask multiple questions in sequence
  - Full graph retrieval path shown
  - Commands: 'save', 'stats', 'sources', 'exit'
  - Conversation history tracking

Example Questions:
  > What are the key requirements of the EU Data Act?
  > How does linked data relate to knowledge graphs?
  > What are the challenges for IoT manufacturers?
  > Explain interoperability standards in the context of data sharing

Expected Time: 15-30 minutes (your research session)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Step 4.3: Use Flask Web UI (Alternative)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Tool: server.py
Command:
  python server.py

What happens:
  1. Starts Flask server on http://localhost:5000
  2. Opens simple chat interface in browser
  3. Note: Need to build graph first or server won't use it

Better approach: Modify server.py to auto-build graph on startup
  (Currently doesn't load graph by default)

Access:
  Open browser: http://localhost:5000
  Chat interface with your knowledge base

Current Limitation: Web UI doesn't display retrieval path
  (Shows answer and sources only, not topics/concepts)

Expected Time: Ongoing (keeps running)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

PHASE 5: KNOWLEDGE SYNTHESIS & ARTICLE GENERATION
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Step 5.1: Generate AI Synthesis Article from Graph
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Tool: generate_article_from_graph.py
Input: data/graphs/knowledge_graph.ttl
Output: data/sources/knowledge_graph_article.md

Command:
  python generate_article_from_graph.py data/graphs/knowledge_graph.ttl

What happens:
  1. Reads RDF graph from TTL file
  2. Extracts all documents, concepts, topics
  3. Uses GPT-4o-mini to synthesize coherent article
  4. Writes narrative combining all sources
  5. Saves with YAML frontmatter

Output File:
  data/sources/knowledge_graph_article.md

Contents:
  - Title: "Understanding the Themes and Concepts in the Knowledge Graph"
  - Comprehensive synthesis of all documents
  - Organized by topics and concepts
  - ~3000-5000 words

Next Step: This article becomes part of your knowledge base!
  - Next time you build graph, it includes the synthesis
  - You can chat with the synthesis ("What are the main themes?")
  - Creates self-reinforcing knowledge loop

Expected Time: 10-15 seconds

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Step 5.2: Rebuild Graph with Synthesis Article
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Tool: build_graph.py
Command:
  python build_graph.py

Why: Now includes knowledge_graph_article.md as a source
  - Graph becomes richer with synthesized insights
  - Enables meta-analysis ("What are the overarching themes?")
  - Closes the knowledge loop

Expected Stats Increase:
  - Documents: +1
  - Chunks: +3-5
  - Concepts: +10-20 (synthesis creates new conceptual links)
  - Triples: +50-100

Expected Time: 5-10 seconds

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

PHASE 6: ADVANCED ANALYSIS & ITERATION
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Step 6.1: Topic-Specific Queries
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Tool: test_chat.py or interactive_chat.py

Example Questions Tailored to EU Data Act Research:

1. Policy & Compliance:
   > What are the compliance requirements for manufacturers under the EU Data Act?
   > How does the Data Act address data portability?
   > What are the interoperability standards required?

2. Technical Implementation:
   > How do knowledge graphs support Data Act compliance?
   > What role does linked data play in data sharing?
   > Explain the relationship between RDF and property graphs

3. Business Impact:
   > What are the challenges for IoT device manufacturers?
   > How does the Data Act affect cloud service providers?
   > What are the timelines for implementation?

4. Standards & Frameworks:
   > Which standardization organizations are involved?
   > What is the role of ETSI, CEN, and CENELEC?
   > How do SAREF and NGSI-LD relate to the Data Act?

Observe:
  - Which topics get activated for each question
  - Which concepts are matched
  - Quality of retrieval path
  - Relevance of chunks retrieved

Expected Time: 20-40 minutes (deep research session)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Step 6.2: Identify Knowledge Gaps
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Tool: Manual analysis + chat queries

Method:
  1. Ask broad question: "What topics are covered in the knowledge base?"
  2. Ask specific question: "What are the privacy implications?"
  3. If answer is weak or missing, note the gap
  4. Return to Phase 2 (Web Discovery) to fill gaps

Example Gaps You Might Find:
  - Specific case studies of Data Act implementation
  - Technical specifications for data formats
  - Legal precedents or interpretations
  - Industry-specific guidance (automotive, healthcare, etc.)

Action: Add new sources and rebuild graph

Expected Time: 15-30 minutes

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Step 6.3: Export Conversation History
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Tool: Interactive chat 'save' command

During chat session:
  > save

Output: conversation_YYYYMMDD_HHMMSS.json

Contents:
  {
    "session_start": "2025-12-09T14:30:00",
    "model": "gpt-4o-mini",
    "num_documents": 19,
    "conversation": [
      {
        "timestamp": "...",
        "question": "What is the EU Data Act?",
        "answer": "...",
        "sources": [...]
      }
    ]
  }

Use Case: Review your research session, extract insights, create notes

Expected Time: Instant

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Step 6.4: Generate Research Summary (Optional)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Tool: features/artifacts.py (if needed for timeline/summary)

Note: artifacts.py has timeline and summary generation features
  - Can create chronological event timelines
  - Generate executive summaries
  - Currently requires integration into workflow

Current Status: Available but not in main pipeline
  Future Enhancement: Add to interactive_chat.py as command

Expected Time: N/A (future feature)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

PHASE 7: ITERATION & REFINEMENT
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Step 7.1: Manual TTL Editing (Advanced)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Tool: Text editor + domain expertise

File: data/graphs/knowledge_graph.ttl

What you can do:
  1. Add missing concepts manually
  2. Add skos:broader/narrower relationships
  3. Merge duplicate concepts
  4. Add domain-specific properties
  5. Correct mislabeled entities

Example Edit:
  # Add hierarchy
  onto:EU_Data_Act a onto:DomainConcept ;
      skos:prefLabel "EU Data Act" ;
      skos:broader onto:EU_Regulations .

  onto:EU_Regulations a onto:DomainConcept ;
      skos:prefLabel "EU Regulations" .

After editing:
  - Graph is now enhanced with expert knowledge
  - Next build_graph.py will overwrite (save backup!)
  - Future: Visual editor will make this easier (Phase 3)

Expected Time: 30-60 minutes (expert knowledge work)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Step 7.2: Add More Sources Iteratively
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Cycle:
  1. Research session (Step 4.2) reveals gaps
  2. Web discovery (Step 2.1) finds new sources
  3. Add to data/sources/
  4. Rebuild graph (Step 3.1)
  5. Generate new synthesis (Step 5.1)
  6. Repeat

This creates a virtuous cycle:
  Research â†’ Gaps â†’ New Sources â†’ Richer Graph â†’ Better Research

Expected Time: Ongoing research process

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Step 7.3: Track Graph Evolution
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Tool: Manual tracking or git

Method:
  # Before adding sources
  python build_graph.py > graph_stats_baseline.txt

  # After adding 5 new sources
  python build_graph.py > graph_stats_v2.txt

  # Compare
  Compare-Object (Get-Content graph_stats_baseline.txt) (Get-Content graph_stats_v2.txt)

Metrics to track:
  - Documents: Î”+5
  - Concepts: Î”+40
  - Topics: Î”+2
  - Triples: Î”+150

Insight: Measure knowledge base growth over time

Expected Time: 5 minutes per checkpoint

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

=================================================================================
COMPLETE WORKFLOW SUMMARY (Quick Reference)
=================================================================================

1. PREPARE SOURCES (One-time per batch)
   âœ“ Convert DOCX to MD/PDF
   âœ“ Add frontmatter
   âœ“ Optional: Web discovery for new sources
   âœ“ Optional: YouTube transcript extraction

2. BUILD GRAPH
   â†’ python build_graph.py
   Output: data/graphs/knowledge_graph.ttl

3. TEST RETRIEVAL
   â†’ python test_chat.py
   Verify: Graph-guided retrieval working

4. RESEARCH SESSION
   â†’ python interactive_chat.py (create this)
   OR
   â†’ python server.py (web UI)
   
   Ask questions, explore topics, identify gaps

5. GENERATE SYNTHESIS
   â†’ python generate_article_from_graph.py data/graphs/knowledge_graph.ttl
   Output: data/sources/knowledge_graph_article.md

6. REBUILD WITH SYNTHESIS
   â†’ python build_graph.py
   Graph now includes synthesized insights

7. ITERATE
   Find gaps â†’ Add sources â†’ Rebuild â†’ Research â†’ Repeat

=================================================================================
TIME ESTIMATES
=================================================================================

First-Time Setup (with DOCX conversion):
  - Phase 1 (Prepare): 15-20 minutes
  - Phase 2 (Discovery): 30-60 minutes (optional, depends on scope)
  - Phase 3 (Build Graph): 10 seconds
  - Phase 4 (First Research): 30-45 minutes
  - Phase 5 (Synthesis): 15 seconds
  TOTAL: 1.5-2.5 hours

Subsequent Iterations (adding sources):
  - Add sources: 5-10 minutes
  - Rebuild graph: 10 seconds
  - Research session: 20-30 minutes
  - Re-synthesis: 15 seconds
  TOTAL: 30-45 minutes per iteration

=================================================================================
TROUBLESHOOTING
=================================================================================

Problem: "No module named 'html2text'"
Solution: pip install html2text

Problem: Graph building fails
Solution: Check verbose output, ensure sources are valid files

Problem: OpenAI API errors
Solution: Verify OPENAI_API_KEY in .env file

Problem: Graph retrieval returns no results
Solution: Ensure graph was built (check len(rag.rdf_graph) > 0)

Problem: DOCX file not loading
Solution: Convert to MD or PDF first (see Phase 1, Step 1.1)

Problem: Web UI doesn't show retrieval path
Solution: Use interactive_chat.py instead (terminal-based has full path display)

=================================================================================
NEXT STEPS: PHASE 2 IMPLEMENTATION (FUTURE)
=================================================================================

Reminder: Add to .github/copilot-instructions.md

Priority 1 Enhancements:
  1. Multi-hop reasoning (concept â†’ related_concept traversal)
  2. Concept hierarchy (skos:broader/narrower extraction)
  3. Performance optimization (pre-compute embeddings, caching)
  4. DOCX file support (python-docx integration)

Priority 2 Enhancements:
  5. Topic-based retrieval (query specifically for topics)
  6. Semantic clustering (embeddings-based, not batch)
  7. LLM-generated topic labels (more meaningful names)

Priority 3 Enhancements:
  8. Visual graph editor (Dash + Cytoscape UI)
  9. Automated feedback loop (usage analytics â†’ graph improvements)
  10. Multi-language support (SKOS labels with @lang tags)

Target: 85/100 compliance after Phase 2
        95/100 compliance after Phase 3

=================================================================================
CONCLUSION
=================================================================================

You now have a complete research pipeline that:
  âœ“ Ingests heterogeneous sources (PDF, HTML, MD, TXT)
  âœ“ Builds semantic knowledge graphs (3-layer architecture)
  âœ“ Uses graph-guided retrieval (topic â†’ concept â†’ chunk)
  âœ“ Provides full transparency (retrieval path logging)
  âœ“ Generates synthesis articles (knowledge loop closure)
  âœ“ Scales to thousands of documents (tested architecture)

Current Status: 75/100 compliance with GraphRAG specification
Next Milestone: Phase 2 implementation â†’ 85/100 compliance

Your immediate action: Convert DOCX â†’ MD, then follow Phase 1-5.

Happy researching! ðŸš€

=================================================================================
