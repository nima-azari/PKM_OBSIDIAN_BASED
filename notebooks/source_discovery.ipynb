{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e3608ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Web Discovery initialized\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from core.web_discovery import WebDiscovery\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize web discovery\n",
    "discovery = WebDiscovery()\n",
    "sources_dir = Path('../data/sources')\n",
    "sources_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"‚úì Web Discovery initialized\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a534c53",
   "metadata": {},
   "source": [
    "## Step 1: Choose Your Approach\n",
    "\n",
    "Select how to guide the source discovery process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db049f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Loading documents from sources directory...\n",
      "\n",
      "  ‚úì Loaded 26 document(s)\n",
      "  Types: 8x.md, 4x.txt, 12x.pdf, 2x.html\n",
      "\n",
      "ü§ñ Extracting research topic with AI...\n",
      "\n",
      "  ‚úì Loaded 26 document(s)\n",
      "  Types: 8x.md, 4x.txt, 12x.pdf, 2x.html\n",
      "\n",
      "ü§ñ Extracting research topic with AI...\n",
      "\n",
      "  ‚úì Extracted topic\n",
      "\n",
      "üéØ Research Topic:\n",
      "   The impact of the EU Data Act on transitioning to Linked Data platforms for data accessibility.\n",
      "\n",
      "   Length: 16 words (target: <15 words)\n",
      "  ‚úì Extracted topic\n",
      "\n",
      "üéØ Research Topic:\n",
      "   The impact of the EU Data Act on transitioning to Linked Data platforms for data accessibility.\n",
      "\n",
      "   Length: 16 words (target: <15 words)\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# CHOOSE YOUR APPROACH: Uncomment ONE of the three options below\n",
    "# ===================================================================\n",
    "\n",
    "# Option 1: Manual research topic\n",
    "# research_topic = \"EU Data Act and Linked Data governance frameworks\"\n",
    "# use_graph_mode = False\n",
    "\n",
    "# Option 2: Auto-extract from existing documents\n",
    "# research_topic = None\n",
    "# use_graph_mode = False\n",
    "\n",
    "# Option 3: Use knowledge graph (recommended if you have a refined graph) ‚≠ê\n",
    "research_topic = None\n",
    "use_graph_mode = True\n",
    "graph_path = \"../data/graphs/knowledge_graph.ttl\"  # Path to your knowledge graph\n",
    "\n",
    "# ===================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "from markitdown import MarkItDown\n",
    "from core.rag_engine import VaultRAG\n",
    "\n",
    "# Initialize MarkItDown for DOCX conversion\n",
    "md_converter = MarkItDown()\n",
    "\n",
    "# Step 1: Convert any DOCX files to Markdown\n",
    "docx_files = list(sources_dir.glob(\"*.docx\"))\n",
    "if docx_files:\n",
    "    print(f\"üìÑ Converting {len(docx_files)} DOCX file(s)...\\n\")\n",
    "    for docx_path in docx_files:\n",
    "        try:\n",
    "            result = md_converter.convert(str(docx_path))\n",
    "            md_path = docx_path.with_suffix('.md')\n",
    "            with open(md_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(result.text_content)\n",
    "            docx_path.unlink()\n",
    "            print(f\"  ‚úì Converted: {md_path.name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚úó Error: {e}\")\n",
    "    print()\n",
    "\n",
    "# Step 2: Load documents and/or knowledge graph\n",
    "if use_graph_mode:\n",
    "    print(\"üîç Graph Mode: Loading knowledge graph...\\n\")\n",
    "    \n",
    "    # Check if graph exists\n",
    "    graph_file = Path(graph_path)\n",
    "    if not graph_file.exists():\n",
    "        print(f\"  ‚ö†Ô∏è Knowledge graph not found at: {graph_path}\")\n",
    "        print(f\"  üí° Run 'python build_graph.py' to create a knowledge graph first\")\n",
    "        print(f\"  Falling back to document extraction mode\\n\")\n",
    "        use_graph_mode = False\n",
    "    else:\n",
    "        # Load the existing knowledge graph\n",
    "        rag = VaultRAG(sources_dir=str(sources_dir), verbose=False)\n",
    "        \n",
    "        # Load graph from TTL file\n",
    "        try:\n",
    "            rag.rdf_graph.parse(str(graph_file), format='turtle')\n",
    "            print(f\"  ‚úì Loaded knowledge graph from {graph_file.name}\")\n",
    "            \n",
    "            # Get graph statistics\n",
    "            stats = rag.get_graph_stats()\n",
    "            print(f\"  üìä Graph contains:\")\n",
    "            print(f\"      - {stats['domain_concepts']} domain concepts\")\n",
    "            print(f\"      - {stats['topic_nodes']} topic nodes\")\n",
    "            print(f\"      - {stats['documents']} documents\")\n",
    "            print(f\"      - {stats['chunks']} chunks\")\n",
    "            print(f\"      - {stats['total_triples']} total triples\\n\")\n",
    "            \n",
    "            # Extract topics and concepts from graph\n",
    "            print(\"üß† Extracting knowledge from graph...\\n\")\n",
    "            topics = rag.get_graph_topics(top_k=5)\n",
    "            concepts = rag.get_graph_concepts(top_k=20)\n",
    "            \n",
    "            if topics:\n",
    "                print(f\"  ‚úì Found {len(topics)} topic nodes:\")\n",
    "                for i, topic in enumerate(topics[:3], 1):\n",
    "                    print(f\"      {i}. {topic['label']}\")\n",
    "                    if topic.get('concepts'):\n",
    "                        print(f\"         Concepts: {', '.join(topic['concepts'][:3])}\")\n",
    "            \n",
    "            if concepts:\n",
    "                print(f\"\\n  ‚úì Found {len(concepts)} domain concepts:\")\n",
    "                print(f\"      {', '.join(concepts[:10])}\")\n",
    "            \n",
    "            print()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚úó Error loading graph: {e}\")\n",
    "            print(f\"  Falling back to document extraction mode\\n\")\n",
    "            use_graph_mode = False\n",
    "\n",
    "# Fall back to document extraction if not using graph\n",
    "if not use_graph_mode and research_topic is None:\n",
    "    print(\"üîç Document Mode: Loading documents from sources directory...\\n\")\n",
    "    \n",
    "    rag = VaultRAG(sources_dir=str(sources_dir), verbose=False)\n",
    "    rag._load_documents()\n",
    "    documents = rag.documents\n",
    "    \n",
    "    if not documents:\n",
    "        print(\"  ‚ö†Ô∏è No documents found in sources directory.\")\n",
    "        print(\"  Using default topic.\\n\")\n",
    "        research_topic = \"EU Data Act and Linked Data governance frameworks\"\n",
    "    else:\n",
    "        print(f\"  ‚úì Loaded {len(documents)} document(s)\")\n",
    "        \n",
    "        # Show document types\n",
    "        doc_types = {}\n",
    "        for doc in documents:\n",
    "            ext = Path(doc.path).suffix\n",
    "            doc_types[ext] = doc_types.get(ext, 0) + 1\n",
    "        \n",
    "        print(f\"  Types: {', '.join([f'{count}x{ext}' for ext, count in doc_types.items()])}\\n\")\n",
    "        \n",
    "        # Extract research topic using WebDiscovery AI\n",
    "        print(\"ü§ñ Extracting research topic with AI...\\n\")\n",
    "        \n",
    "        # Combine first 3 documents (up to 2000 chars each)\n",
    "        combined_content = \"\\n\\n---\\n\\n\".join([\n",
    "            f\"Document {i+1}: {doc.title}\\n{doc.content[:2000]}\"\n",
    "            for i, doc in enumerate(documents[:3])\n",
    "        ])\n",
    "        \n",
    "        # Use WebDiscovery's research topic extraction\n",
    "        research_topic = discovery.extract_research_topic(combined_content, max_words=15)\n",
    "        \n",
    "        print(f\"  ‚úì Extracted topic\")\n",
    "\n",
    "# Display summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "if use_graph_mode:\n",
    "    print(\"üìå MODE: Knowledge Graph-Guided Discovery\")\n",
    "    print(f\"   Using {len(topics)} topics and {len(concepts)} concepts from your refined graph\")\n",
    "else:\n",
    "    print(\"üìå MODE: Document-Based Discovery\")\n",
    "    print(f\"üéØ Research Topic: {research_topic}\")\n",
    "    print(f\"   Length: {len(research_topic.split())} words\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e35ace",
   "metadata": {},
   "source": [
    "## Step 2: Generate Search Queries\n",
    "\n",
    "AI will generate optimized search queries for your topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4566a44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Searching for articles automatically...\n",
      "\n",
      "üìÇ Checking existing sources...\n",
      "  Found 1 existing sources to skip\n",
      "\n",
      "üìù Generating targeted search queries...\n",
      "  Generated 5 queries:\n",
      "\n",
      "    1. \"EU Data Act impact on Linked Data platforms for data accessibility\"\n",
      "    2. \"transitioning to Linked Data platforms under EU Data Act regulations\"\n",
      "    3. \"effects of EU Data Act on data accessibility in Linked Data environments\"\n",
      "    4. \"EU Data Act implications for Linked Data implementation and data sharing\"\n",
      "    5. \"challenges of adopting Linked Data platforms post EU Data Act enactment\"\n",
      "\n",
      "[1/5] Searching: \"EU Data Act impact on Linked Data platforms for data accessibility\"...\n",
      "  Generated 5 queries:\n",
      "\n",
      "    1. \"EU Data Act impact on Linked Data platforms for data accessibility\"\n",
      "    2. \"transitioning to Linked Data platforms under EU Data Act regulations\"\n",
      "    3. \"effects of EU Data Act on data accessibility in Linked Data environments\"\n",
      "    4. \"EU Data Act implications for Linked Data implementation and data sharing\"\n",
      "    5. \"challenges of adopting Linked Data platforms post EU Data Act enactment\"\n",
      "\n",
      "[1/5] Searching: \"EU Data Act impact on Linked Data platforms for data accessibility\"...\n",
      "  ‚úì Found 0 results (0 new)\n",
      "  ‚úì Found 0 results (0 new)\n",
      "[2/5] Searching: \"transitioning to Linked Data platforms under EU Data Act regulations\"...\n",
      "[2/5] Searching: \"transitioning to Linked Data platforms under EU Data Act regulations\"...\n",
      "  ‚úì Found 3 results (3 new)\n",
      "  ‚úì Found 3 results (3 new)\n",
      "[3/5] Searching: \"effects of EU Data Act on data accessibility in Linked Data environme...\n",
      "[3/5] Searching: \"effects of EU Data Act on data accessibility in Linked Data environme...\n",
      "  ‚úì Found 3 results (3 new)\n",
      "  ‚úì Found 3 results (3 new)\n",
      "[4/5] Searching: \"EU Data Act implications for Linked Data implementation and data shar...\n",
      "[4/5] Searching: \"EU Data Act implications for Linked Data implementation and data shar...\n",
      "  ‚úì Found 3 results (3 new)\n",
      "  ‚úì Found 3 results (3 new)\n",
      "[5/5] Searching: \"challenges of adopting Linked Data platforms post EU Data Act enactme...\n",
      "[5/5] Searching: \"challenges of adopting Linked Data platforms post EU Data Act enactme...\n",
      "  ‚úì Found 0 results (0 new)\n",
      "\n",
      "‚úÖ Search complete: 8 unique new results\n",
      "\n",
      "üìö Search Results:\n",
      "\n",
      "[1] Understanding switching rights under the Data Act...\n",
      "    Source: Google\n",
      "    https://www.dlapiper.com/en/insights/publications/2025/07/understandin...\n",
      "    With the EU Data Act set to become applicable, our alert discusses termination rights when switching between data proces...\n",
      "\n",
      "[2] Simpler EU digital rules and new digital wallets to save ......\n",
      "    Source: Google\n",
      "    https://ec.europa.eu/commission/presscorner/detail/en/ip_25_2718...\n",
      "    Introducing targeted exemptions to some of the Data Act's cloud-switching rules for SMEs and SMCs resulting in around ‚Ç¨1...\n",
      "\n",
      "[3] EU Data Act Significant New Switching Requirements Due ......\n",
      "    Source: Google\n",
      "    https://www.lw.com/en/insights/eu-data-act-significant-new-switching-r...\n",
      "    One key aspect of the Data Act is the introduction of significant new service-switching requirements on providers of ‚Äúda...\n",
      "\n",
      "[4] The impact of the EU Data Act on data processing services ......\n",
      "    Source: Google\n",
      "    https://www.bclplaw.com/en-US/events-insights-news/the-impact-of-the-e...\n",
      "    The Data Act became applicable from 12 September 2025 to all entities within its scope. However, the Data Act also intro...\n",
      "\n",
      "[5] EU Data Act operational impacts: Balancing risks and ......\n",
      "    Source: Google\n",
      "    https://iapp.org/news/a/eu-data-act-operational-impacts-balancing-risk...\n",
      "    The Data Act aims to make industrial data more accessible and usable. It seeks to promote fair conditions in allocating ...\n",
      "\n",
      "[6] Data Act explained | Shaping Europe's digital future...\n",
      "    Source: Google\n",
      "    https://digital-strategy.ec.europa.eu/en/factpages/data-act-explained...\n",
      "    The Data Act is a law designed to enhance the EU's data economy and foster a competitive data market by making data (in ...\n",
      "\n",
      "[7] Data Act | Shaping Europe's digital future - European Union...\n",
      "    Source: Google\n",
      "    https://digital-strategy.ec.europa.eu/en/policies/data-act...\n",
      "    The Data Act gives individuals and businesses the right to access the data produced through their utilisation of smart o...\n",
      "\n",
      "[8] EU Data Act: A new era for data sharing has begun...\n",
      "    Source: Google\n",
      "    https://www.cliffordchance.com/insights/resources/blogs/talking-tech/e...\n",
      "    Main aims of the EU Data Act in respect of data access and re-use ¬∑ Enhancing transparency ¬∑ Ensuring fairness in data s...\n",
      "\n",
      "üí° Proceed to Step 3 to select which articles to extract\n",
      "  ‚úì Found 0 results (0 new)\n",
      "\n",
      "‚úÖ Search complete: 8 unique new results\n",
      "\n",
      "üìö Search Results:\n",
      "\n",
      "[1] Understanding switching rights under the Data Act...\n",
      "    Source: Google\n",
      "    https://www.dlapiper.com/en/insights/publications/2025/07/understandin...\n",
      "    With the EU Data Act set to become applicable, our alert discusses termination rights when switching between data proces...\n",
      "\n",
      "[2] Simpler EU digital rules and new digital wallets to save ......\n",
      "    Source: Google\n",
      "    https://ec.europa.eu/commission/presscorner/detail/en/ip_25_2718...\n",
      "    Introducing targeted exemptions to some of the Data Act's cloud-switching rules for SMEs and SMCs resulting in around ‚Ç¨1...\n",
      "\n",
      "[3] EU Data Act Significant New Switching Requirements Due ......\n",
      "    Source: Google\n",
      "    https://www.lw.com/en/insights/eu-data-act-significant-new-switching-r...\n",
      "    One key aspect of the Data Act is the introduction of significant new service-switching requirements on providers of ‚Äúda...\n",
      "\n",
      "[4] The impact of the EU Data Act on data processing services ......\n",
      "    Source: Google\n",
      "    https://www.bclplaw.com/en-US/events-insights-news/the-impact-of-the-e...\n",
      "    The Data Act became applicable from 12 September 2025 to all entities within its scope. However, the Data Act also intro...\n",
      "\n",
      "[5] EU Data Act operational impacts: Balancing risks and ......\n",
      "    Source: Google\n",
      "    https://iapp.org/news/a/eu-data-act-operational-impacts-balancing-risk...\n",
      "    The Data Act aims to make industrial data more accessible and usable. It seeks to promote fair conditions in allocating ...\n",
      "\n",
      "[6] Data Act explained | Shaping Europe's digital future...\n",
      "    Source: Google\n",
      "    https://digital-strategy.ec.europa.eu/en/factpages/data-act-explained...\n",
      "    The Data Act is a law designed to enhance the EU's data economy and foster a competitive data market by making data (in ...\n",
      "\n",
      "[7] Data Act | Shaping Europe's digital future - European Union...\n",
      "    Source: Google\n",
      "    https://digital-strategy.ec.europa.eu/en/policies/data-act...\n",
      "    The Data Act gives individuals and businesses the right to access the data produced through their utilisation of smart o...\n",
      "\n",
      "[8] EU Data Act: A new era for data sharing has begun...\n",
      "    Source: Google\n",
      "    https://www.cliffordchance.com/insights/resources/blogs/talking-tech/e...\n",
      "    Main aims of the EU Data Act in respect of data access and re-use ¬∑ Enhancing transparency ¬∑ Ensuring fairness in data s...\n",
      "\n",
      "üí° Proceed to Step 3 to select which articles to extract\n"
     ]
    }
   ],
   "source": [
    "# Automated search with improved query generation\n",
    "print(\"üîç Generating search queries...\\n\")\n",
    "\n",
    "# Load existing sources to avoid duplicates\n",
    "print(\"üìÇ Checking existing sources...\")\n",
    "existing_urls = set()\n",
    "\n",
    "for md_file in sources_dir.glob('*.md'):\n",
    "    try:\n",
    "        with open(md_file, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "            import re\n",
    "            url_match = re.search(r'url:\\s*(.+)', content)\n",
    "            if url_match:\n",
    "                existing_urls.add(url_match.group(1).strip())\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(f\"  Found {len(existing_urls)} existing sources to skip\\n\")\n",
    "\n",
    "# Generate search queries based on mode\n",
    "if use_graph_mode:\n",
    "    print(\"üß† Generating queries from knowledge graph concepts...\")\n",
    "    queries = discovery.generate_queries_from_graph_concepts(\n",
    "        topics=topics,\n",
    "        concepts=concepts,\n",
    "        num_queries=5\n",
    "    )\n",
    "else:\n",
    "    print(\"üìù Generating queries from research topic...\")\n",
    "    queries = discovery._generate_search_queries(research_topic)\n",
    "\n",
    "print(f\"\\n  ‚úì Generated {len(queries)} targeted queries:\\n\")\n",
    "for i, q in enumerate(queries, 1):\n",
    "    print(f\"    {i}. {q}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üí° NEXT STEPS:\")\n",
    "print(\"   1. Copy each query above\")\n",
    "print(\"   2. Search on Google Scholar, arXiv, or your preferred source\")\n",
    "print(\"   3. Copy relevant article URLs\")\n",
    "print(\"   4. Paste URLs in the next cell\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3768eee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Understanding switching rights under the Data Act\n",
      "Simpler EU digital rules and new digital wallets to save ...\n",
      "EU Data Act Significant New Switching Requirements Due ...\n",
      "The impact of the EU Data Act on data processing services ...\n",
      "EU Data Act operational impacts: Balancing risks and ...\n",
      "Data Act explained | Shaping Europe's digital future\n",
      "Data Act | Shaping Europe's digital future - European Union\n",
      "EU Data Act: A new era for data sharing has begun\n"
     ]
    }
   ],
   "source": [
    "for result in all_results:\n",
    "    print(result['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8f6a19",
   "metadata": {},
   "source": [
    "## Step 3: Select Articles to Extract\n",
    "\n",
    "Choose which search results to download and process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9e3135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select articles to extract\n",
    "# Options:\n",
    "#   1. Extract all: selected_indices = list(range(1, len(all_results) + 1))\n",
    "#   2. Extract first N: selected_indices = list(range(1, 6))\n",
    "#   3. Select specific: selected_indices = [1, 3, 5, 7, 10]\n",
    "\n",
    "if len(all_results) == 0:\n",
    "    print(\"‚ö†Ô∏è No search results available.\")\n",
    "    print(\"   Run Step 2 first, or add manual URLs below.\")\n",
    "    selected_indices = []\n",
    "else:\n",
    "    # Default: First 5 results\n",
    "    selected_indices = list(range(1, min(6, len(all_results) + 1)))\n",
    "    \n",
    "    print(f\"üìã Selected {len(selected_indices)} articles:\\n\")\n",
    "    for i in selected_indices:\n",
    "        if i <= len(all_results):\n",
    "            result = all_results[i-1]\n",
    "            print(f\"[{i}] {result['title'][:70]}...\")\n",
    "            print(f\"    {result['url'][:75]}...\")\n",
    "            print()\n",
    "\n",
    "# Add manual URLs if needed\n",
    "manual_urls = []\n",
    "\"\"\"\n",
    "# Uncomment to add specific URLs not in search results:\n",
    "manual_urls = [\n",
    "    \"https://example.com/article1\",\n",
    "    \"https://example.com/article2\",\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "if selected_indices:\n",
    "    print(f\"üí° Edit selected_indices above to change selection\")\n",
    "if manual_urls:\n",
    "    print(f\"   + {len(manual_urls)} manual URLs will be added\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77277cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build final URL list from selected results + manual URLs\n",
    "urls = []\n",
    "\n",
    "# Add selected search results\n",
    "for i in selected_indices:\n",
    "    if i <= len(all_results):\n",
    "        urls.append(all_results[i-1]['url'])\n",
    "\n",
    "# Add manual URLs if any\n",
    "urls.extend(manual_urls)\n",
    "\n",
    "# Remove duplicates while preserving order\n",
    "seen = set()\n",
    "unique_urls = []\n",
    "for url in urls:\n",
    "    if url not in seen and url not in existing_urls:\n",
    "        seen.add(url)\n",
    "        unique_urls.append(url)\n",
    "\n",
    "urls = unique_urls\n",
    "\n",
    "if len(urls) == 0:\n",
    "    print(\"‚ö†Ô∏è No URLs selected. Run Step 3 to select articles.\")\n",
    "else:\n",
    "    print(f\"‚úì Ready to extract {len(urls)} articles:\\n\")\n",
    "    for i, url in enumerate(urls, 1):\n",
    "        print(f\"{i}. {url[:80]}...\")\n",
    "    print(f\"\\nüí° Proceed to Step 4 to extract content\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee43833",
   "metadata": {},
   "source": [
    "## Step 4: Extract and Save Articles\n",
    "\n",
    "Download article content, assess quality, and save high-quality sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46c51fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract, assess, and auto-save high-quality articles\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "if len(urls) == 0:\n",
    "    print(\"‚ö†Ô∏è No URLs to process. Run Steps 2-3 first.\")\n",
    "else:\n",
    "    print(f\"üîç Extracting {len(urls)} articles...\\n\")\n",
    "    \n",
    "    saved_count = 0\n",
    "    skipped_count = 0\n",
    "    failed_count = 0\n",
    "    \n",
    "    for i, url in enumerate(urls, 1):\n",
    "        print(f\"[{i}/{len(urls)}] {url[:65]}...\")\n",
    "        \n",
    "        try:\n",
    "            # Extract article content\n",
    "            article = discovery.extract_article(url)\n",
    "            \n",
    "            if not article:\n",
    "                print(f\"  ‚úó Extraction failed\")\n",
    "                failed_count += 1\n",
    "                continue\n",
    "            \n",
    "            # Assess quality with AI\n",
    "            assessment = discovery.assess_quality(article)\n",
    "            quality_score = assessment.get('quality_score', 0)\n",
    "            \n",
    "            print(f\"  üìÑ {article['title'][:60]}...\")\n",
    "            print(f\"  üë§ {article.get('author', 'Unknown')}\")\n",
    "            print(f\"  üìè {len(article['content'])} chars\")\n",
    "            print(f\"  ‚≠ê Quality: {quality_score}/10\")\n",
    "            \n",
    "            # Save if quality meets threshold (‚â•6)\n",
    "            if quality_score >= 6:\n",
    "                # Create safe filename\n",
    "                safe_title = re.sub(r'[^\\w\\s-]', '', article['title'])\n",
    "                safe_title = re.sub(r'[-\\s]+', '-', safe_title)[:100]\n",
    "                filename = f\"{safe_title}.md\"\n",
    "                filepath = sources_dir / filename\n",
    "                \n",
    "                # Build markdown with frontmatter\n",
    "                content = f\"\"\"---\n",
    "title: {article['title']}\n",
    "author: {article.get('author', 'Unknown')}\n",
    "url: {article['url']}\n",
    "date_extracted: {datetime.now().strftime('%Y-%m-%d')}\n",
    "quality_score: {quality_score}\n",
    "tags: [web-article, research, {research_topic.lower().replace(' ', '-')[:30]}]\n",
    "---\n",
    "\n",
    "# {article['title']}\n",
    "\n",
    "**Author:** {article.get('author', 'Unknown')}  \n",
    "**Source:** {article['url']}  \n",
    "**Extracted:** {datetime.now().strftime('%Y-%m-%d')}  \n",
    "**Quality:** {quality_score}/10\n",
    "\n",
    "---\n",
    "\n",
    "{article['content']}\n",
    "\"\"\"\n",
    "                \n",
    "                # Write to file\n",
    "                with open(filepath, 'w', encoding='utf-8') as f:\n",
    "                    f.write(content)\n",
    "                \n",
    "                print(f\"  ‚úì Saved: {filename}\")\n",
    "                saved_count += 1\n",
    "            else:\n",
    "                print(f\"  ‚äò Skipped: Quality {quality_score} < 6\")\n",
    "                skipped_count += 1\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚úó Error: {str(e)[:60]}...\")\n",
    "            failed_count += 1\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    # Summary\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"‚úÖ Extraction Complete\\n\")\n",
    "    print(f\"   Saved:   {saved_count} high-quality articles (‚â•6/10)\")\n",
    "    print(f\"   Skipped: {skipped_count} low-quality articles (<6/10)\")\n",
    "    print(f\"   Failed:  {failed_count} extraction errors\")\n",
    "    print(f\"   Total:   {len(urls)} articles processed\")\n",
    "    print(f\"\\n   Location: {sources_dir.resolve()}\")\n",
    "    \n",
    "    if saved_count > 0:\n",
    "        print(f\"\\nüîÑ Next Steps:\")\n",
    "        print(f\"   1. Build knowledge graph:\")\n",
    "        print(f\"      python build_graph.py\")\n",
    "        print(f\"   2. Start chat interface:\")\n",
    "        print(f\"      python server.py\")\n",
    "        print(f\"   3. Generate synthesis article:\")\n",
    "        print(f\"      python generate_article_from_graph.py data/graphs/knowledge_graph.ttl\")\n",
    "    else:\n",
    "        print(f\"\\nüí° No articles saved. Try:\")\n",
    "        print(f\"   ‚Ä¢ Lower quality threshold in code (change 'if quality_score >= 6')\")\n",
    "        print(f\"   ‚Ä¢ Select different search results\")\n",
    "        print(f\"   ‚Ä¢ Adjust research topic to be more specific\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
