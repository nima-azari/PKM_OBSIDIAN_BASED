{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e3608ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Web Discovery initialized\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from core.web_discovery import WebDiscovery\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize web discovery\n",
    "discovery = WebDiscovery()\n",
    "sources_dir = Path('../data/sources')\n",
    "sources_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"âœ“ Web Discovery initialized\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a534c53",
   "metadata": {},
   "source": [
    "## Step 1: Enter Research Topic\n",
    "\n",
    "What topic are you researching?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db049f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Processing: LinkedIn+Blog_Posts_Set_1_of_3_EU_Data_Act_Linked_Data.docx\n",
      "Loading documents from ..\\data\\sources...\n",
      "Found 19 files\n",
      "  âœ“ Loaded: Understanding the Themes and Concepts in the Knowledge Graph\n",
      "  âœ“ Loaded: Understanding the Themes and Concepts in the Knowledge Graph\n",
      "  âœ“ Loaded: YouTube-8nbb6CwpPMA\n",
      "  âœ“ Loaded: YouTube Links to Process\n",
      "  âœ“ Loaded: RDF Levels the Advantages of Labeled Property Graphs & Keeps Three Key Benefits_ Standards, Semantics & Interoperability _ Ontotext_files\\f(1).txt\n",
      "  âœ“ Loaded: RDF Levels the Advantages of Labeled Property Graphs & Keeps Three Key Benefits_ Standards, Semantics & Interoperability _ Ontotext_files\\f(2).txt\n",
      "  âœ“ Loaded: RDF Levels the Advantages of Labeled Property Graphs & Keeps Three Key Benefits_ Standards, Semantics & Interoperability _ Ontotext_files\\f.txt\n",
      "  âœ“ Loaded: Cloud and Edge Computing_ a different way of using IT â€” Brochure _ Shaping Europeâ€™s digital future.pdf\n",
      "  âœ“ Loaded: Cloud switching under the EU Data Act.pdf\n",
      "  âœ“ Loaded: Cloud switching under the EU Data Act.pdf\n",
      "  âœ“ Loaded: Knowledge Graphs_ Redefining Data Management for the Modern Enterprise.pdf\n",
      "  âœ“ Loaded: Knowledge Graphs_ Redefining Data Management for the Modern Enterprise.pdf\n",
      "  âœ“ Loaded: node_12633_printable_pdf.pdf\n",
      "  âœ“ Loaded: node_12633_printable_pdf.pdf\n",
      "  âœ“ Loaded: The EU Data Act explained_ rights, obligations and challenges for data holders and manufacturers.pdf\n",
      "  âœ“ Loaded: The EU Data Act explained_ rights, obligations and challenges for data holders and manufacturers.pdf\n",
      "  âœ“ Loaded: tr_104410v010101p.pdf\n",
      "  âœ“ Loaded: tr_104410v010101p.pdf\n",
      "  âœ“ Loaded: RDF Levels the Advantages of Labeled Property Graphs & Keeps Three Key Benefits: Standards, Semantics & Interoperability\n",
      "  âœ“ Loaded: RDF Levels the Advantages of Labeled Property Graphs & Keeps Three Key Benefits_ Standards, Semantics & Interoperability _ Ontotext_files\\saved_resource(1).html\n",
      "  âœ“ Loaded: RDF Levels the Advantages of Labeled Property Graphs & Keeps Three Key Benefits_ Standards, Semantics & Interoperability _ Ontotext_files\\saved_resource(2).html\n",
      "  âœ“ Loaded: RDF Levels the Advantages of Labeled Property Graphs & Keeps Three Key Benefits_ Standards, Semantics & Interoperability _ Ontotext_files\\saved_resource.html\n",
      "  âœ“ Loaded: RDF Levels the Advantages of Labeled Property Graphs & Keeps Three Key Benefits_ Standards, Semantics & Interoperability _ Ontotext_files\\sm.25.html\n",
      "  âœ“ Converted DOCX â†’ TXT: LinkedIn+Blog_Posts_Set_1_of_3_EU_Data_Act_Linked_Data.txt\n",
      "  âœ“ Loaded: LinkedIn+Blog_Posts_Set_1_of_3_EU_Data_Act_Linked_Data.docx\n",
      "Loaded 19 documents\n",
      "\n",
      "  Using existing TXT: LinkedIn+Blog_Posts_Set_1_of_3_EU_Data_Act_Linked_Data.txt\n",
      "âœ“ Converted to: LinkedIn+Blog_Posts_Set_1_of_3_EU_Data_Act_Linked_Data.txt\n",
      "ðŸ“Š Content length: 7695 characters\n",
      "\n",
      "ðŸ“– First 500 characters:\n",
      "Set 1 out of 3: Why, How, and Differences (Setting the Ground)\n",
      "\n",
      "Collection Name: Why Linked Data Infrastructure and Wistor app in the age of EU Data Act?\n",
      "\n",
      "Intro\n",
      "\n",
      "Set 1 focuses on the crucial link between the EU Data Act mandate and the necessity of shifting from traditional cloud management (like AWS/Azure/GCP) to a native Linked Data platform like Wistor.\n",
      "\n",
      "LinkedIn Post Draft (Approx. 350 words)\n",
      "\n",
      "Hook (Using Template: Contrarian Approach): Stop asking how to store more data on AWS/Azure. Start \n",
      "\n",
      "  âœ“ Loaded: RDF Levels the Advantages of Labeled Property Graphs & Keeps Three Key Benefits: Standards, Semantics & Interoperability\n",
      "  âœ“ Loaded: RDF Levels the Advantages of Labeled Property Graphs & Keeps Three Key Benefits_ Standards, Semantics & Interoperability _ Ontotext_files\\saved_resource(1).html\n",
      "  âœ“ Loaded: RDF Levels the Advantages of Labeled Property Graphs & Keeps Three Key Benefits_ Standards, Semantics & Interoperability _ Ontotext_files\\saved_resource(2).html\n",
      "  âœ“ Loaded: RDF Levels the Advantages of Labeled Property Graphs & Keeps Three Key Benefits_ Standards, Semantics & Interoperability _ Ontotext_files\\saved_resource.html\n",
      "  âœ“ Loaded: RDF Levels the Advantages of Labeled Property Graphs & Keeps Three Key Benefits_ Standards, Semantics & Interoperability _ Ontotext_files\\sm.25.html\n",
      "  âœ“ Converted DOCX â†’ TXT: LinkedIn+Blog_Posts_Set_1_of_3_EU_Data_Act_Linked_Data.txt\n",
      "  âœ“ Loaded: LinkedIn+Blog_Posts_Set_1_of_3_EU_Data_Act_Linked_Data.docx\n",
      "Loaded 19 documents\n",
      "\n",
      "  Using existing TXT: LinkedIn+Blog_Posts_Set_1_of_3_EU_Data_Act_Linked_Data.txt\n",
      "âœ“ Converted to: LinkedIn+Blog_Posts_Set_1_of_3_EU_Data_Act_Linked_Data.txt\n",
      "ðŸ“Š Content length: 7695 characters\n",
      "\n",
      "ðŸ“– First 500 characters:\n",
      "Set 1 out of 3: Why, How, and Differences (Setting the Ground)\n",
      "\n",
      "Collection Name: Why Linked Data Infrastructure and Wistor app in the age of EU Data Act?\n",
      "\n",
      "Intro\n",
      "\n",
      "Set 1 focuses on the crucial link between the EU Data Act mandate and the necessity of shifting from traditional cloud management (like AWS/Azure/GCP) to a native Linked Data platform like Wistor.\n",
      "\n",
      "LinkedIn Post Draft (Approx. 350 words)\n",
      "\n",
      "Hook (Using Template: Contrarian Approach): Stop asking how to store more data on AWS/Azure. Start \n",
      "\n",
      "\n",
      "ðŸŽ¯ Research topic: **Main Research Questions and Topics:**\n",
      "1. How does the EU Data Act impact data ownership and accessibility for individuals, businesses, and public organizations?\n",
      "2. What are the limitations of traditional cloud management systems (AWS, Azure, GCP) in meeting the requirements of the EU Data Act?\n",
      "3. How can organizations transition from traditional cloud infrastructures to a Linked Data platform like Wistor to ensure compliance and enhance data interoperability?\n",
      "4. What role does semantic interoperability play in the context of the EU Data Act and data sharing?\n",
      "\n",
      "**Concise Research Topic Statement:**\n",
      "This research explores the implications of the EU Data Act on data ownership and accessibility, emphasizing the need for a strategic shift from traditional cloud management systems to Linked Data platforms to achieve semantic interoperability and compliance.\n",
      "\n",
      "ðŸŽ¯ Research topic: **Main Research Questions and Topics:**\n",
      "1. How does the EU Data Act impact data ownership and accessibility for individuals, businesses, and public organizations?\n",
      "2. What are the limitations of traditional cloud management systems (AWS, Azure, GCP) in meeting the requirements of the EU Data Act?\n",
      "3. How can organizations transition from traditional cloud infrastructures to a Linked Data platform like Wistor to ensure compliance and enhance data interoperability?\n",
      "4. What role does semantic interoperability play in the context of the EU Data Act and data sharing?\n",
      "\n",
      "**Concise Research Topic Statement:**\n",
      "This research explores the implications of the EU Data Act on data ownership and accessibility, emphasizing the need for a strategic shift from traditional cloud management systems to Linked Data platforms to achieve semantic interoperability and compliance.\n"
     ]
    }
   ],
   "source": [
    "# # Option 1: Enter your research topic manually\n",
    "# research_topic = \"Knowledge graphs and semantic web technologies\"\n",
    "\n",
    "# Option 2: Extract research questions from a DOCX file\n",
    "from core.rag_engine import VaultRAG\n",
    "from pathlib import Path\n",
    "\n",
    "docx_path = Path(\"../data/sources/LinkedIn+Blog_Posts_Set_1_of_3_EU_Data_Act_Linked_Data.docx\")\n",
    "\n",
    "# print(f\"ðŸ“„ Processing: {docx_path.name}\")\n",
    "\n",
    "# Initialize RAG to convert DOCX to TXT\n",
    "rag = VaultRAG(sources_dir=\"../data/sources\", verbose=True)\n",
    "\n",
    "# Convert DOCX to TXT\n",
    "txt_path = rag._convert_docx_to_txt(docx_path)\n",
    "\n",
    "if txt_path and txt_path.exists():\n",
    "    print(f\"âœ“ Converted to: {txt_path.name}\")\n",
    "    \n",
    "    # Read the TXT file\n",
    "    with open(txt_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    print(f\"ðŸ“Š Content length: {len(content)} characters\")\n",
    "    print(f\"\\nðŸ“– First 500 characters:\\n{content[:500]}\\n\")\n",
    "    \n",
    "    if len(content.strip()) < 100:\n",
    "        print(\"âš ï¸ Warning: Content too short. Using fallback topic.\")\n",
    "        research_topic = \"EU Data Act and Linked Data governance\"\n",
    "    else:\n",
    "        # Use AI to extract main research questions\n",
    "        from openai import OpenAI\n",
    "        client = OpenAI()\n",
    "        \n",
    "        # Use first 5000 characters for extraction\n",
    "        content_sample = content[:5000]\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a research assistant. Extract the main research questions and topics from the given document. Return a concise research topic statement (one sentence) that captures the core themes.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Extract the main research topic from this document:\\n\\n{content_sample}\"}\n",
    "            ],\n",
    "            temperature=0.3\n",
    "        )\n",
    "        \n",
    "        research_topic = response.choices[0].message.content.strip()\n",
    "else:\n",
    "    print(\"âœ— Failed to convert DOCX. Using fallback topic.\")\n",
    "    research_topic = \"EU Data Act and Linked Data governance\"\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Research topic: {research_topic}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e35ace",
   "metadata": {},
   "source": [
    "## Step 2: Generate Search Queries\n",
    "\n",
    "AI will generate optimized search queries for your topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4566a44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ Generated Search Queries:\n",
      "\n",
      "1. 1. \"impact of EU Data Act on data ownership and accessibility for individuals and businesses\"\n",
      "2. 2. \"limitations of traditional cloud management systems in compliance with EU Data Act\"\n",
      "3. 3. \"transitioning from traditional cloud infrastructure to Linked Data platforms for EU Data Act compliance\"\n",
      "4. 4. \"semantic interoperability and the EU Data Act implications for data sharing\"\n",
      "5. 5. \"EU Data Act and its effects on data accessibility in public organizations\"\n",
      "\n",
      "ðŸ’¡ Copy these queries and search on:\n",
      "   - Google Scholar\n",
      "   - arXiv\n",
      "   - Medium\n",
      "   - Academic journals\n"
     ]
    }
   ],
   "source": [
    "# Option 1: Manual search (generate queries, you search manually)\n",
    "# Uncomment to use:\n",
    "\"\"\"\n",
    "queries = discovery._generate_search_queries(research_topic)\n",
    "\n",
    "print(\"\\nðŸ“ Generated Search Queries:\\n\")\n",
    "for i, query in enumerate(queries, 1):\n",
    "    print(f\"{i}. {query}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Copy these queries and search on:\")\n",
    "print(\"   - Google Scholar\")\n",
    "print(\"   - arXiv\")\n",
    "print(\"   - Medium\")\n",
    "print(\"   - Academic journals\")\n",
    "\"\"\"\n",
    "\n",
    "# Option 2: Automated search (uses free APIs)\n",
    "print(\"ðŸ” Searching for articles automatically...\\n\")\n",
    "\n",
    "# Generate search queries\n",
    "queries = discovery._generate_search_queries(research_topic)\n",
    "print(f\"ðŸ“ Generated {len(queries)} search queries\\n\")\n",
    "\n",
    "# Search using free APIs (arXiv + Semantic Scholar)\n",
    "all_results = []\n",
    "\n",
    "for i, query in enumerate(queries, 1):\n",
    "    print(f\"[{i}/{len(queries)}] Searching: {query}\")\n",
    "    \n",
    "    # Search academic sources (FREE - no API key needed)\n",
    "    results = discovery.search_web(query, max_results=5, source='all')\n",
    "    \n",
    "    print(f\"  âœ“ Found {len(results)} results\")\n",
    "    all_results.extend(results)\n",
    "\n",
    "print(f\"\\nâœ… Total results found: {len(all_results)}\")\n",
    "\n",
    "# Display results with indices\n",
    "print(\"\\nðŸ“š Search Results:\\n\")\n",
    "for i, result in enumerate(all_results[:20], 1):  # Show first 20\n",
    "    print(f\"[{i}] {result['title']}\")\n",
    "    print(f\"    Source: {result['source']}\")\n",
    "    print(f\"    URL: {result['url']}\")\n",
    "    print(f\"    Snippet: {result['snippet'][:150]}...\")\n",
    "    print()\n",
    "\n",
    "print(f\"\\nðŸ’¡ Next step: Copy the URLs you want to extract to Step 3 below.\")\n",
    "print(f\"   Or manually add more URLs from Google Scholar, Medium, etc.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8f6a19",
   "metadata": {},
   "source": [
    "## Step 3: Paste URLs\n",
    "\n",
    "After searching, paste the URLs you want to extract (one per line)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77277cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Use automated search results\n",
    "# Select which search results to extract (by index number from above)\n",
    "selected_indices = [1, 2, 3, 4, 5]  # Change these to select specific results\n",
    "\n",
    "# Extract URLs from selected results\n",
    "urls = [all_results[i-1]['url'] for i in selected_indices if i <= len(all_results)]\n",
    "\n",
    "print(f\"ðŸ“‹ Selected {len(urls)} URLs from search results:\\n\")\n",
    "for i, url in enumerate(urls, 1):\n",
    "    print(f\"{i}. {url}\")\n",
    "\n",
    "# Option B: Manually paste URLs (uncomment to use)\n",
    "\"\"\"\n",
    "urls_text = '''\n",
    "https://example.com/article1\n",
    "https://example.com/article2\n",
    "https://example.com/article3\n",
    "'''\n",
    "\n",
    "urls = [url.strip() for url in urls_text.strip().split('\\n') if url.strip()]\n",
    "\n",
    "print(f\"\\nðŸ“‹ Found {len(urls)} URLs to process\")\n",
    "for i, url in enumerate(urls, 1):\n",
    "    print(f\"{i}. {url}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee43833",
   "metadata": {},
   "source": [
    "## Step 4: Extract Articles\n",
    "\n",
    "Extract content from URLs and preview quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ee93a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract articles\n",
    "articles = []\n",
    "\n",
    "print(\"\\nðŸ” Extracting articles...\\n\")\n",
    "\n",
    "for i, url in enumerate(urls, 1):\n",
    "    print(f\"[{i}/{len(urls)}] Processing: {url}\")\n",
    "    \n",
    "    try:\n",
    "        article = discovery.extract_article(url)\n",
    "        \n",
    "        if article:\n",
    "            # Get quality assessment\n",
    "            assessment = discovery.assess_quality(article)\n",
    "            article['assessment'] = assessment\n",
    "            articles.append(article)\n",
    "            \n",
    "            print(f\"  âœ“ {article['title']}\")\n",
    "            print(f\"    Author: {article.get('author', 'Unknown')}\")\n",
    "            print(f\"    Length: {len(article['content'])} chars\")\n",
    "            print(f\"    Quality: {assessment.get('quality_score', 'N/A')}\")\n",
    "        else:\n",
    "            print(f\"  âœ— Could not extract article\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— Error: {str(e)}\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(f\"\\nâœ“ Successfully extracted {len(articles)} articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd64c68",
   "metadata": {},
   "source": [
    "## Step 5: Review and Filter\n",
    "\n",
    "Review extracted articles and select which ones to save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b31c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display articles with indices\n",
    "print(\"\\nðŸ“š Extracted Articles:\\n\")\n",
    "\n",
    "for i, article in enumerate(articles):\n",
    "    print(f\"[{i}] {article['title']}\")\n",
    "    print(f\"    URL: {article['url']}\")\n",
    "    print(f\"    Author: {article.get('author', 'Unknown')}\")\n",
    "    print(f\"    Length: {len(article['content'])} characters\")\n",
    "    \n",
    "    # Show assessment if available\n",
    "    if 'assessment' in article:\n",
    "        assessment = article['assessment']\n",
    "        print(f\"    Quality: {assessment.get('quality_score', 'N/A')}\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(\"\\nðŸ’¡ To save specific articles, set indices_to_save below.\")\n",
    "print(\"   Example: indices_to_save = [0, 1, 3]  # Save articles 0, 1, and 3\")\n",
    "print(\"   Or leave empty to save all: indices_to_save = []\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23496be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select which articles to save (empty list = save all)\n",
    "indices_to_save = []  # Change this to select specific articles\n",
    "\n",
    "# If empty, save all\n",
    "if not indices_to_save:\n",
    "    articles_to_save = articles\n",
    "else:\n",
    "    articles_to_save = [articles[i] for i in indices_to_save if i < len(articles)]\n",
    "\n",
    "print(f\"Selected {len(articles_to_save)} articles to save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beacbbeb",
   "metadata": {},
   "source": [
    "## Step 6: Save to Sources Directory\n",
    "\n",
    "Save selected articles as markdown files in `data/sources/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68d28f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save articles\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "saved_files = []\n",
    "\n",
    "print(\"\\nðŸ’¾ Saving articles...\\n\")\n",
    "\n",
    "for article in articles_to_save:\n",
    "    # Create safe filename\n",
    "    safe_title = re.sub(r'[^\\w\\s-]', '', article['title'])\n",
    "    safe_title = re.sub(r'[-\\s]+', '-', safe_title)[:100]\n",
    "    filename = f\"{safe_title}.md\"\n",
    "    filepath = sources_dir / filename\n",
    "    \n",
    "    # Create markdown content with frontmatter\n",
    "    content = f\"\"\"---\n",
    "title: {article['title']}\n",
    "author: {article.get('author', 'Unknown')}\n",
    "url: {article['url']}\n",
    "date_extracted: {datetime.now().strftime('%Y-%m-%d')}\n",
    "tags: [web-article, {research_topic.lower().replace(' ', '-')}]\n",
    "---\n",
    "\n",
    "# {article['title']}\n",
    "\n",
    "**Author:** {article.get('author', 'Unknown')}  \n",
    "**Source:** {article['url']}  \n",
    "**Extracted:** {datetime.now().strftime('%Y-%m-%d')}\n",
    "\n",
    "---\n",
    "\n",
    "{article['content']}\n",
    "\"\"\"\n",
    "    \n",
    "    # Write file\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        f.write(content)\n",
    "    \n",
    "    saved_files.append(filepath)\n",
    "    print(f\"  âœ“ Saved: {filename}\")\n",
    "\n",
    "print(f\"\\nâœ… Successfully saved {len(saved_files)} articles to data/sources/\")\n",
    "print(\"\\nðŸ”„ Next steps:\")\n",
    "print(\"   1. Launch the UI: python server.py\")\n",
    "print(\"   2. Ask questions about your new sources!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
